{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "from chromadb.config import Settings\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import ollama\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database\n",
    "client = chromadb.PersistentClient(path=\"./chroma_vectorDB\", settings=Settings(anonymized_telemetry=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader to load from directory\n",
    "loader = DirectoryLoader(path=\"./docs\", glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the docs into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "chunked_docs = text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_docs_list = [item.page_content for item in chunked_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked_docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brijeshsingh/chatbot/chat_env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = embedding_model.embed_documents(chunked_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(document_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create collection\n",
    "collection = client.create_collection(name=\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert documents, embedding along with uuids into the collection:\n",
    "collection.add(ids=[f\"{uuid.uuid4()}\" for _ in range(len(chunked_docs_list))], documents=chunked_docs_list, embeddings=document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = \"Explain Energy Minimization Network.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedding_model.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = collection.query(query_embeddings=query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Given the query, '{query}' and the context, '{query_result['documents'][0][0]}'. Answer the query on the basis of given context or use your intelligence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model='gemma:2b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content':prompt,\n",
    "        \"format\": \"json\",\n",
    "        \"stream\": False,\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's the answer:\n",
      "\n",
      "The Energy Minimization Network (EMN) is a machine learning model that aims to learn relationships between sub-sequences of words by minimizing the energy function E. The energy function takes in a sub-sequence of words (mapped to their feature vectors) and outputs an energy value, which measures how likely it is for that sub-sequence to appear in a natural language text.\n",
      "\n",
      "EMN utilizes a feature vector representation for the output words, which allows it to consider the semantic and syntactic similarities between different words. This is in contrast to previous models that relied solely on lexical representations.\n",
      "\n",
      "The EMN also takes into account the conditional probabilities of the output words, represented by the vector of biases b. These biases are learned during the training process and contribute to the model's ability to accurately predict the energy function.\n",
      "\n",
      "EMN can be viewed as a non-normalized log-probability measure, where the energy function assigns higher probabilities to sub-sequences that are more likely to appear in a natural language text.\n"
     ]
    }
   ],
   "source": [
    "# Parse the JSON string\n",
    "json_data = json.loads(json.dumps(response))\n",
    "# Access the 'content' field\n",
    "content = json_data['message']['content']\n",
    "# Print the content in a readable format\n",
    "print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
